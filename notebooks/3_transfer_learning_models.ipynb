{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, XLNetTokenizer, XLNetForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "df_reviews = pd.read_csv(\"../data/processed/df_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_reviews\u001b[38;5;241m.\u001b[39mshape()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "df_reviews.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45000</th>\n",
       "      <td>positive</td>\n",
       "      <td>ive worked classic several party say despite l...</td>\n",
       "      <td>['ive', 'worked', 'classic', 'several', 'party...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45001</th>\n",
       "      <td>positive</td>\n",
       "      <td>best theater ever digital screen loyalty progr...</td>\n",
       "      <td>['best', 'theater', 'ever', 'digital', 'screen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45002</th>\n",
       "      <td>positive</td>\n",
       "      <td>ive ever came breakfast couple time even thoug...</td>\n",
       "      <td>['ive', 'ever', 'came', 'breakfast', 'couple',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45003</th>\n",
       "      <td>positive</td>\n",
       "      <td>could eat every day go spurt eat several meal ...</td>\n",
       "      <td>['could', 'eat', 'every', 'day', 'go', 'spurt'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45004</th>\n",
       "      <td>positive</td>\n",
       "      <td>staff friendly coffee perfect doughnut always ...</td>\n",
       "      <td>['staff', 'friendly', 'coffee', 'perfect', 'do...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text  \\\n",
       "45000  positive  ive worked classic several party say despite l...   \n",
       "45001  positive  best theater ever digital screen loyalty progr...   \n",
       "45002  positive  ive ever came breakfast couple time even thoug...   \n",
       "45003  positive  could eat every day go spurt eat several meal ...   \n",
       "45004  positive  staff friendly coffee perfect doughnut always ...   \n",
       "\n",
       "                                                  tokens  \n",
       "45000  ['ive', 'worked', 'classic', 'several', 'party...  \n",
       "45001  ['best', 'theater', 'ever', 'digital', 'screen...  \n",
       "45002  ['ive', 'ever', 'came', 'breakfast', 'couple',...  \n",
       "45003  ['could', 'eat', 'every', 'day', 'go', 'spurt'...  \n",
       "45004  ['staff', 'friendly', 'coffee', 'perfect', 'do...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_reviews['text'], df_reviews['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization for RoBERTa\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "train_encodings_roberta = tokenizer_roberta(X_train.tolist(), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "test_encodings_roberta = tokenizer_roberta(X_test.tolist(), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization for XLNet\n",
    "tokenizer_xlnet = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "train_encodings_xlnet = tokenizer_xlnet(X_train.tolist(), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "test_encodings_xlnet = tokenizer_xlnet(X_test.tolist(), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numerical values\n",
    "label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "y_train_numeric = y_train.map(label_map).tolist()\n",
    "y_test_numeric = y_test.map(label_map).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset_roberta = CustomDataset(train_encodings_roberta, y_train_numeric)\n",
    "test_dataset_roberta = CustomDataset(test_encodings_roberta, y_test_numeric)\n",
    "\n",
    "train_dataset_xlnet = CustomDataset(train_encodings_xlnet, y_train_numeric)\n",
    "test_dataset_xlnet = CustomDataset(test_encodings_xlnet, y_test_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define the RoBERTa model\n",
    "model_roberta = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define the XLNet model\n",
    "model_xlnet = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../models/transfer_learning/roberta_model',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='../logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer for RoBERTa\n",
    "trainer_roberta = Trainer(\n",
    "    model=model_roberta,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_roberta,\n",
    "    eval_dataset=test_dataset_roberta,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer for XLNet\n",
    "trainer_xlnet = Trainer(\n",
    "    model=model_xlnet,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_xlnet,\n",
    "    eval_dataset=test_dataset_xlnet,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1de3b037f9545b4b17a41e1aeb368f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1188, 'grad_norm': 3.486030101776123, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.04}\n",
      "{'loss': 1.1078, 'grad_norm': 3.9940593242645264, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.08}\n",
      "{'loss': 1.0737, 'grad_norm': 3.5862300395965576, 'learning_rate': 3e-06, 'epoch': 0.12}\n",
      "{'loss': 1.0266, 'grad_norm': 4.154654026031494, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.16}\n",
      "{'loss': 0.8674, 'grad_norm': 3.6694533824920654, 'learning_rate': 5e-06, 'epoch': 0.2}\n",
      "{'loss': 0.8099, 'grad_norm': 7.234017848968506, 'learning_rate': 6e-06, 'epoch': 0.24}\n",
      "{'loss': 0.7151, 'grad_norm': 3.921678304672241, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.28}\n",
      "{'loss': 0.6218, 'grad_norm': 8.32940673828125, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.32}\n",
      "{'loss': 0.732, 'grad_norm': 12.056319236755371, 'learning_rate': 9e-06, 'epoch': 0.36}\n",
      "{'loss': 0.6139, 'grad_norm': 20.39013671875, 'learning_rate': 1e-05, 'epoch': 0.4}\n",
      "{'loss': 0.5167, 'grad_norm': 5.1688232421875, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.44}\n",
      "{'loss': 0.6934, 'grad_norm': 65.26964569091797, 'learning_rate': 1.2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.5555, 'grad_norm': 32.7700309753418, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.52}\n",
      "{'loss': 0.4655, 'grad_norm': 9.67892837524414, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.56}\n",
      "{'loss': 0.4816, 'grad_norm': 7.2727131843566895, 'learning_rate': 1.5e-05, 'epoch': 0.6}\n",
      "{'loss': 0.6746, 'grad_norm': 5.9180450439453125, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6123, 'grad_norm': 10.923042297363281, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.68}\n",
      "{'loss': 0.4896, 'grad_norm': 34.216102600097656, 'learning_rate': 1.8e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6582, 'grad_norm': 17.4951229095459, 'learning_rate': 1.9e-05, 'epoch': 0.76}\n",
      "{'loss': 0.4487, 'grad_norm': 10.487556457519531, 'learning_rate': 2e-05, 'epoch': 0.8}\n",
      "{'loss': 0.6401, 'grad_norm': 13.168848037719727, 'learning_rate': 2.1e-05, 'epoch': 0.84}\n",
      "{'loss': 0.5356, 'grad_norm': 9.315242767333984, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.88}\n",
      "{'loss': 0.4848, 'grad_norm': 10.372398376464844, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.92}\n",
      "{'loss': 0.5461, 'grad_norm': 11.586630821228027, 'learning_rate': 2.4e-05, 'epoch': 0.96}\n",
      "{'loss': 0.6101, 'grad_norm': 10.745315551757812, 'learning_rate': 2.5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33b3d708b1e4b90a28da6c0630f0005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6459236741065979, 'eval_runtime': 190.199, 'eval_samples_per_second': 5.258, 'eval_steps_per_second': 0.331, 'epoch': 1.0}\n",
      "{'loss': 0.5459, 'grad_norm': 14.623492240905762, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.04}\n",
      "{'loss': 0.4476, 'grad_norm': 6.672553062438965, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.08}\n",
      "{'loss': 0.5544, 'grad_norm': 14.787919998168945, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.12}\n",
      "{'loss': 0.543, 'grad_norm': 3.346604108810425, 'learning_rate': 2.9e-05, 'epoch': 1.16}\n",
      "{'loss': 0.4405, 'grad_norm': 3.739861249923706, 'learning_rate': 3e-05, 'epoch': 1.2}\n",
      "{'loss': 0.5562, 'grad_norm': 29.010997772216797, 'learning_rate': 3.1e-05, 'epoch': 1.24}\n",
      "{'loss': 0.4257, 'grad_norm': 8.847997665405273, 'learning_rate': 3.2000000000000005e-05, 'epoch': 1.28}\n",
      "{'loss': 0.3417, 'grad_norm': 11.799880981445312, 'learning_rate': 3.3e-05, 'epoch': 1.32}\n",
      "{'loss': 0.5539, 'grad_norm': 14.800016403198242, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.36}\n",
      "{'loss': 0.4983, 'grad_norm': 8.84719181060791, 'learning_rate': 3.5e-05, 'epoch': 1.4}\n",
      "{'loss': 0.3998, 'grad_norm': 19.320175170898438, 'learning_rate': 3.6e-05, 'epoch': 1.44}\n",
      "{'loss': 0.3933, 'grad_norm': 23.139972686767578, 'learning_rate': 3.7e-05, 'epoch': 1.48}\n",
      "{'loss': 0.4544, 'grad_norm': 18.55768585205078, 'learning_rate': 3.8e-05, 'epoch': 1.52}\n",
      "{'loss': 0.4165, 'grad_norm': 40.16468811035156, 'learning_rate': 3.9000000000000006e-05, 'epoch': 1.56}\n",
      "{'loss': 0.5896, 'grad_norm': 10.84529972076416, 'learning_rate': 4e-05, 'epoch': 1.6}\n",
      "{'loss': 0.5578, 'grad_norm': 5.988813400268555, 'learning_rate': 4.1e-05, 'epoch': 1.64}\n",
      "{'loss': 0.4692, 'grad_norm': 10.057289123535156, 'learning_rate': 4.2e-05, 'epoch': 1.68}\n",
      "{'loss': 0.5077, 'grad_norm': 28.076555252075195, 'learning_rate': 4.3e-05, 'epoch': 1.72}\n",
      "{'loss': 0.5854, 'grad_norm': 16.437177658081055, 'learning_rate': 4.4000000000000006e-05, 'epoch': 1.76}\n",
      "{'loss': 0.4996, 'grad_norm': 5.79238748550415, 'learning_rate': 4.5e-05, 'epoch': 1.8}\n",
      "{'loss': 0.3886, 'grad_norm': 29.19228172302246, 'learning_rate': 4.600000000000001e-05, 'epoch': 1.84}\n",
      "{'loss': 0.5704, 'grad_norm': 10.36825180053711, 'learning_rate': 4.7e-05, 'epoch': 1.88}\n",
      "{'loss': 0.491, 'grad_norm': 4.4798808097839355, 'learning_rate': 4.8e-05, 'epoch': 1.92}\n",
      "{'loss': 0.4099, 'grad_norm': 5.876469612121582, 'learning_rate': 4.9e-05, 'epoch': 1.96}\n",
      "{'loss': 0.5214, 'grad_norm': 8.243544578552246, 'learning_rate': 5e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8923104c029048be8ed93097c0735ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3971913456916809, 'eval_runtime': 123.0126, 'eval_samples_per_second': 8.129, 'eval_steps_per_second': 0.512, 'epoch': 2.0}\n",
      "{'loss': 0.3807, 'grad_norm': 2.025066375732422, 'learning_rate': 4.8e-05, 'epoch': 2.04}\n",
      "{'loss': 0.4677, 'grad_norm': 4.267898082733154, 'learning_rate': 4.600000000000001e-05, 'epoch': 2.08}\n",
      "{'loss': 0.4534, 'grad_norm': 10.444662094116211, 'learning_rate': 4.4000000000000006e-05, 'epoch': 2.12}\n",
      "{'loss': 0.4358, 'grad_norm': 12.122603416442871, 'learning_rate': 4.2e-05, 'epoch': 2.16}\n",
      "{'loss': 0.3377, 'grad_norm': 12.173665046691895, 'learning_rate': 4e-05, 'epoch': 2.2}\n",
      "{'loss': 0.3194, 'grad_norm': 8.490181922912598, 'learning_rate': 3.8e-05, 'epoch': 2.24}\n",
      "{'loss': 0.5062, 'grad_norm': 29.916885375976562, 'learning_rate': 3.6e-05, 'epoch': 2.28}\n",
      "{'loss': 0.271, 'grad_norm': 6.563568592071533, 'learning_rate': 3.4000000000000007e-05, 'epoch': 2.32}\n",
      "{'loss': 0.3356, 'grad_norm': 16.56073570251465, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.36}\n",
      "{'loss': 0.3591, 'grad_norm': 7.858523368835449, 'learning_rate': 3e-05, 'epoch': 2.4}\n",
      "{'loss': 0.2916, 'grad_norm': 7.064492702484131, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.44}\n",
      "{'loss': 0.4287, 'grad_norm': 12.006505012512207, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.48}\n",
      "{'loss': 0.4671, 'grad_norm': 7.818596363067627, 'learning_rate': 2.4e-05, 'epoch': 2.52}\n",
      "{'loss': 0.3397, 'grad_norm': 14.000103950500488, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.56}\n",
      "{'loss': 0.3655, 'grad_norm': 34.5081901550293, 'learning_rate': 2e-05, 'epoch': 2.6}\n",
      "{'loss': 0.2614, 'grad_norm': 3.5454306602478027, 'learning_rate': 1.8e-05, 'epoch': 2.64}\n",
      "{'loss': 0.3694, 'grad_norm': 0.9301881790161133, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.68}\n",
      "{'loss': 0.3491, 'grad_norm': 27.79865264892578, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.72}\n",
      "{'loss': 0.4694, 'grad_norm': 67.80046081542969, 'learning_rate': 1.2e-05, 'epoch': 2.76}\n",
      "{'loss': 0.4175, 'grad_norm': 2.442186117172241, 'learning_rate': 1e-05, 'epoch': 2.8}\n",
      "{'loss': 0.3723, 'grad_norm': 28.698881149291992, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.84}\n",
      "{'loss': 0.4093, 'grad_norm': 33.438392639160156, 'learning_rate': 6e-06, 'epoch': 2.88}\n",
      "{'loss': 0.3847, 'grad_norm': 27.26818084716797, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.92}\n",
      "{'loss': 0.2537, 'grad_norm': 3.0002636909484863, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.96}\n",
      "{'loss': 0.3288, 'grad_norm': 22.589492797851562, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b959a29bc94bbe973eec144386d415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4276874363422394, 'eval_runtime': 152.0758, 'eval_samples_per_second': 6.576, 'eval_steps_per_second': 0.414, 'epoch': 3.0}\n",
      "{'train_runtime': 8738.3413, 'train_samples_per_second': 1.373, 'train_steps_per_second': 0.086, 'train_loss': 0.5151433836619059, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.5151433836619059, metrics={'train_runtime': 8738.3413, 'train_samples_per_second': 1.373, 'train_steps_per_second': 0.086, 'total_flos': 789340253184000.0, 'train_loss': 0.5151433836619059, 'epoch': 3.0})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the models\n",
    "trainer_roberta.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230c0d5f03e2418299515372e2f62330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1046, 'grad_norm': 29.230403900146484, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.04}\n",
      "{'loss': 1.041, 'grad_norm': 22.902469635009766, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.08}\n",
      "{'loss': 0.9183, 'grad_norm': 21.140338897705078, 'learning_rate': 3e-06, 'epoch': 0.12}\n",
      "{'loss': 0.927, 'grad_norm': 20.103660583496094, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.16}\n",
      "{'loss': 0.7971, 'grad_norm': 25.162105560302734, 'learning_rate': 5e-06, 'epoch': 0.2}\n",
      "{'loss': 0.8154, 'grad_norm': 25.732370376586914, 'learning_rate': 6e-06, 'epoch': 0.24}\n",
      "{'loss': 0.7503, 'grad_norm': 23.569629669189453, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.28}\n",
      "{'loss': 0.6209, 'grad_norm': 22.90118408203125, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.32}\n",
      "{'loss': 0.8284, 'grad_norm': 11.556195259094238, 'learning_rate': 9e-06, 'epoch': 0.36}\n",
      "{'loss': 0.7, 'grad_norm': 8.88903522491455, 'learning_rate': 1e-05, 'epoch': 0.4}\n",
      "{'loss': 0.5854, 'grad_norm': 21.151744842529297, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.44}\n",
      "{'loss': 0.8044, 'grad_norm': 15.998172760009766, 'learning_rate': 1.2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.6611, 'grad_norm': 21.744747161865234, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.52}\n",
      "{'loss': 0.4909, 'grad_norm': 12.774801254272461, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5447, 'grad_norm': 4.862232685089111, 'learning_rate': 1.5e-05, 'epoch': 0.6}\n",
      "{'loss': 0.6852, 'grad_norm': 26.056819915771484, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.64}\n",
      "{'loss': 0.7239, 'grad_norm': 11.185970306396484, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.68}\n",
      "{'loss': 0.5416, 'grad_norm': 47.558616638183594, 'learning_rate': 1.8e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6745, 'grad_norm': 15.989225387573242, 'learning_rate': 1.9e-05, 'epoch': 0.76}\n",
      "{'loss': 0.5102, 'grad_norm': 11.503937721252441, 'learning_rate': 2e-05, 'epoch': 0.8}\n",
      "{'loss': 0.5855, 'grad_norm': 18.430578231811523, 'learning_rate': 2.1e-05, 'epoch': 0.84}\n",
      "{'loss': 0.562, 'grad_norm': 24.08595085144043, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.88}\n",
      "{'loss': 0.5819, 'grad_norm': 7.086541175842285, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.92}\n",
      "{'loss': 0.5336, 'grad_norm': 7.070479869842529, 'learning_rate': 2.4e-05, 'epoch': 0.96}\n",
      "{'loss': 0.4863, 'grad_norm': 20.503145217895508, 'learning_rate': 2.5e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.49109795689582825, 'eval_model_preparation_time': 0.022, 'eval_runtime': 212.995, 'eval_samples_per_second': 4.695, 'eval_steps_per_second': 0.296, 'epoch': 1.0}\n",
      "{'loss': 0.5168, 'grad_norm': 9.63763427734375, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.04}\n",
      "{'loss': 0.587, 'grad_norm': 8.470481872558594, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.08}\n",
      "{'loss': 0.5292, 'grad_norm': 11.776280403137207, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.12}\n",
      "{'loss': 0.604, 'grad_norm': 7.988471031188965, 'learning_rate': 2.9e-05, 'epoch': 1.16}\n",
      "{'loss': 0.4136, 'grad_norm': 2.15901255607605, 'learning_rate': 3e-05, 'epoch': 1.2}\n",
      "{'loss': 0.4839, 'grad_norm': 27.856285095214844, 'learning_rate': 3.1e-05, 'epoch': 1.24}\n",
      "{'loss': 0.3349, 'grad_norm': 6.023838520050049, 'learning_rate': 3.2000000000000005e-05, 'epoch': 1.28}\n",
      "{'loss': 0.3349, 'grad_norm': 13.602535247802734, 'learning_rate': 3.3e-05, 'epoch': 1.32}\n",
      "{'loss': 0.6589, 'grad_norm': 16.526222229003906, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.36}\n"
     ]
    }
   ],
   "source": [
    "# Train the models\n",
    "trainer_xlnet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c729f8d90d448ff8fddfbf002815302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate RoBERTa\n",
    "predictions_roberta = trainer_roberta.predict(test_dataset_roberta)\n",
    "y_pred_roberta = np.argmax(predictions_roberta.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a2f826f7aa4327883f0ae698271872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate XLNet\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m predictions_xlnet \u001b[38;5;241m=\u001b[39m trainer_xlnet\u001b[38;5;241m.\u001b[39mpredict(test_dataset_xlnet)\n\u001b[0;32m      3\u001b[0m y_pred_xlnet \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions_xlnet\u001b[38;5;241m.\u001b[39mpredictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:4053\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4050\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   4052\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 4053\u001b[0m output \u001b[38;5;241m=\u001b[39m eval_loop(\n\u001b[0;32m   4054\u001b[0m     test_dataloader, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys, metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix\n\u001b[0;32m   4055\u001b[0m )\n\u001b[0;32m   4056\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:4169\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4166\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[0;32m   4168\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 4169\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_step(model, inputs, prediction_loss_only, ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys)\n\u001b[0;32m   4170\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4171\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4173\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:4385\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[0;32m   4383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[0;32m   4384\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 4385\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs, return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   4386\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m   4388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:1543\u001b[0m, in \u001b[0;36mXLNetForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, labels, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1535\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1540\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1541\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1543\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m   1544\u001b[0m     input_ids,\n\u001b[0;32m   1545\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1546\u001b[0m     mems\u001b[38;5;241m=\u001b[39mmems,\n\u001b[0;32m   1547\u001b[0m     perm_mask\u001b[38;5;241m=\u001b[39mperm_mask,\n\u001b[0;32m   1548\u001b[0m     target_mapping\u001b[38;5;241m=\u001b[39mtarget_mapping,\n\u001b[0;32m   1549\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1550\u001b[0m     input_mask\u001b[38;5;241m=\u001b[39minput_mask,\n\u001b[0;32m   1551\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1552\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1553\u001b[0m     use_mems\u001b[38;5;241m=\u001b[39muse_mems,\n\u001b[0;32m   1554\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1555\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1556\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1558\u001b[0m )\n\u001b[0;32m   1559\u001b[0m output \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1561\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_summary(output)\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:1233\u001b[0m, in \u001b[0;36mXLNetModel.forward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m   1231\u001b[0m     hidden_states\u001b[38;5;241m.\u001b[39mappend((output_h, output_g) \u001b[38;5;28;01mif\u001b[39;00m output_g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m output_h)\n\u001b[1;32m-> 1233\u001b[0m outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m   1234\u001b[0m     output_h,\n\u001b[0;32m   1235\u001b[0m     output_g,\n\u001b[0;32m   1236\u001b[0m     attn_mask_h\u001b[38;5;241m=\u001b[39mnon_tgt_mask,\n\u001b[0;32m   1237\u001b[0m     attn_mask_g\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[0;32m   1238\u001b[0m     r\u001b[38;5;241m=\u001b[39mpos_emb,\n\u001b[0;32m   1239\u001b[0m     seg_mat\u001b[38;5;241m=\u001b[39mseg_mat,\n\u001b[0;32m   1240\u001b[0m     mems\u001b[38;5;241m=\u001b[39mmems[i],\n\u001b[0;32m   1241\u001b[0m     target_mapping\u001b[38;5;241m=\u001b[39mtarget_mapping,\n\u001b[0;32m   1242\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask[i],\n\u001b[0;32m   1243\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1244\u001b[0m )\n\u001b[0;32m   1245\u001b[0m output_h, output_g \u001b[38;5;241m=\u001b[39m outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:522\u001b[0m, in \u001b[0;36mXLNetLayer.forward\u001b[1;34m(self, output_h, output_g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     output_g \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    520\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, output_g\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m output_h \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, output_h)\n\u001b[0;32m    524\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (output_h, output_g) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Add again attentions if there are there\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\transformers\\pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(\u001b[38;5;241m*\u001b[39minput_tensors)\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:528\u001b[0m, in \u001b[0;36mXLNetLayer.ff_chunk\u001b[1;34m(self, output_x)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mff_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, output_x):\n\u001b[1;32m--> 528\u001b[0m     output_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff(output_x)\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_x\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:473\u001b[0m, in \u001b[0;36mXLNetFeedForward.forward\u001b[1;34m(self, inp)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp):\n\u001b[0;32m    472\u001b[0m     output \u001b[38;5;241m=\u001b[39m inp\n\u001b[1;32m--> 473\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_1(output)\n\u001b[0;32m    474\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_function(output)\n\u001b[0;32m    475\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(output)\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluate XLNet\n",
    "predictions_xlnet = trainer_xlnet.predict(test_dataset_xlnet)\n",
    "y_pred_xlnet = np.argmax(predictions_xlnet.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions\n",
    "np.save('../models/transfer_learning/roberta_predictions.npy', y_pred_roberta)\n",
    "np.save('../models/transfer_learning/xlnet_predictions.npy', y_pred_xlnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "def plot_training_loss(trainer, model_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(trainer.state.log_history, label='Training Loss')\n",
    "    plt.title(f'{model_name} Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_loss(trainer_roberta, 'RoBERTa')\n",
    "plot_training_loss(trainer_xlnet, 'XLNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evaluation metrics\n",
    "def plot_evaluation_metrics(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    metrics = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "        'Score': [accuracy, precision, recall, f1]\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=metrics, x='Metric', y='Score')\n",
    "    plt.title(f'{model_name} Evaluation Metrics')\n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Score')\n",
    "    plt.show()\n",
    "\n",
    "plot_evaluation_metrics(y_test_numeric, y_pred_roberta, 'RoBERTa')\n",
    "plot_evaluation_metrics(y_test_numeric, y_pred_xlnet, 'XLNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
