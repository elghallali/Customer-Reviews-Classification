{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, XLNetTokenizer, XLNetForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "df_reviews = pd.read_csv(\"../Data/processed/df_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = df_reviews.tail(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_reviews['text'], df_reviews['sentiment'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization for RoBERTa\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "train_encodings_roberta = tokenizer_roberta(X_train.tolist(), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "test_encodings_roberta = tokenizer_roberta(X_test.tolist(), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization for XLNet\n",
    "tokenizer_xlnet = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "train_encodings_xlnet = tokenizer_xlnet(X_train.tolist(), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "test_encodings_xlnet = tokenizer_xlnet(X_test.tolist(), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numerical values\n",
    "label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "y_train_numeric = y_train.map(label_map).tolist()\n",
    "y_test_numeric = y_test.map(label_map).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset_roberta = CustomDataset(train_encodings_roberta, y_train_numeric)\n",
    "test_dataset_roberta = CustomDataset(test_encodings_roberta, y_test_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_xlnet = CustomDataset(train_encodings_xlnet, y_train_numeric)\n",
    "test_dataset_xlnet = CustomDataset(test_encodings_xlnet, y_test_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define the RoBERTa model\n",
    "model_roberta = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define the XLNet model\n",
    "model_xlnet = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdel\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer for RoBERTa\n",
    "trainer_roberta = Trainer(\n",
    "    model=model_roberta,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_roberta,\n",
    "    eval_dataset=test_dataset_roberta,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer for XLNet\n",
    "trainer_xlnet = Trainer(\n",
    "    model=model_xlnet,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_xlnet,\n",
    "    eval_dataset=test_dataset_xlnet,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06714050537468a8e8c9c7820fdf3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.13, 'grad_norm': 3.7246458530426025, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.04}\n",
      "{'loss': 1.127, 'grad_norm': 4.371917724609375, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.08}\n",
      "{'loss': 1.104, 'grad_norm': 3.795968532562256, 'learning_rate': 3e-06, 'epoch': 0.12}\n",
      "{'loss': 1.0625, 'grad_norm': 4.1021037101745605, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.16}\n",
      "{'loss': 0.9453, 'grad_norm': 3.269951105117798, 'learning_rate': 5e-06, 'epoch': 0.2}\n",
      "{'loss': 0.8366, 'grad_norm': 6.965307712554932, 'learning_rate': 6e-06, 'epoch': 0.24}\n",
      "{'loss': 0.7191, 'grad_norm': 4.5477399826049805, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.28}\n",
      "{'loss': 0.6046, 'grad_norm': 8.081356048583984, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.32}\n",
      "{'loss': 0.7518, 'grad_norm': 18.291521072387695, 'learning_rate': 9e-06, 'epoch': 0.36}\n",
      "{'loss': 0.6408, 'grad_norm': 12.932360649108887, 'learning_rate': 1e-05, 'epoch': 0.4}\n",
      "{'loss': 0.5482, 'grad_norm': 6.961044788360596, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.44}\n",
      "{'loss': 0.6702, 'grad_norm': 31.261518478393555, 'learning_rate': 1.2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.5672, 'grad_norm': 27.93047523498535, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.52}\n",
      "{'loss': 0.4286, 'grad_norm': 8.995351791381836, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5091, 'grad_norm': 7.014697551727295, 'learning_rate': 1.5e-05, 'epoch': 0.6}\n",
      "{'loss': 0.7217, 'grad_norm': 6.818578243255615, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.64}\n",
      "{'loss': 0.6322, 'grad_norm': 24.92898941040039, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.68}\n",
      "{'loss': 0.502, 'grad_norm': 35.24092102050781, 'learning_rate': 1.8e-05, 'epoch': 0.72}\n",
      "{'loss': 0.6069, 'grad_norm': 24.48317527770996, 'learning_rate': 1.9e-05, 'epoch': 0.76}\n",
      "{'loss': 0.4512, 'grad_norm': 10.065512657165527, 'learning_rate': 2e-05, 'epoch': 0.8}\n",
      "{'loss': 0.6277, 'grad_norm': 21.333877563476562, 'learning_rate': 2.1e-05, 'epoch': 0.84}\n",
      "{'loss': 0.6392, 'grad_norm': 23.413612365722656, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.88}\n",
      "{'loss': 0.5527, 'grad_norm': 4.707144737243652, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.92}\n",
      "{'loss': 0.5396, 'grad_norm': 6.947994709014893, 'learning_rate': 2.4e-05, 'epoch': 0.96}\n",
      "{'loss': 0.5975, 'grad_norm': 20.83017349243164, 'learning_rate': 2.5e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fe11ae44784c4aaba3f02cd5210dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5057488083839417, 'eval_runtime': 124.0961, 'eval_samples_per_second': 8.058, 'eval_steps_per_second': 0.508, 'epoch': 1.0}\n",
      "{'loss': 0.4619, 'grad_norm': 10.760321617126465, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.04}\n",
      "{'loss': 0.4254, 'grad_norm': 4.914262294769287, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.08}\n",
      "{'loss': 0.5158, 'grad_norm': 21.053640365600586, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.12}\n",
      "{'loss': 0.5562, 'grad_norm': 15.478263854980469, 'learning_rate': 2.9e-05, 'epoch': 1.16}\n",
      "{'loss': 0.373, 'grad_norm': 6.173689842224121, 'learning_rate': 3e-05, 'epoch': 1.2}\n",
      "{'loss': 0.3819, 'grad_norm': 23.08915138244629, 'learning_rate': 3.1e-05, 'epoch': 1.24}\n",
      "{'loss': 0.378, 'grad_norm': 15.639979362487793, 'learning_rate': 3.2000000000000005e-05, 'epoch': 1.28}\n",
      "{'loss': 0.37, 'grad_norm': 21.638898849487305, 'learning_rate': 3.3e-05, 'epoch': 1.32}\n",
      "{'loss': 0.5545, 'grad_norm': 9.286733627319336, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.36}\n",
      "{'loss': 0.5015, 'grad_norm': 9.24675464630127, 'learning_rate': 3.5e-05, 'epoch': 1.4}\n",
      "{'loss': 0.4293, 'grad_norm': 11.887001991271973, 'learning_rate': 3.6e-05, 'epoch': 1.44}\n",
      "{'loss': 0.4084, 'grad_norm': 16.17650604248047, 'learning_rate': 3.7e-05, 'epoch': 1.48}\n",
      "{'loss': 0.4283, 'grad_norm': 15.924552917480469, 'learning_rate': 3.8e-05, 'epoch': 1.52}\n",
      "{'loss': 0.5851, 'grad_norm': 27.965694427490234, 'learning_rate': 3.9000000000000006e-05, 'epoch': 1.56}\n",
      "{'loss': 0.654, 'grad_norm': 4.9797139167785645, 'learning_rate': 4e-05, 'epoch': 1.6}\n",
      "{'loss': 0.4639, 'grad_norm': 21.324642181396484, 'learning_rate': 4.1e-05, 'epoch': 1.64}\n",
      "{'loss': 0.5016, 'grad_norm': 19.664602279663086, 'learning_rate': 4.2e-05, 'epoch': 1.68}\n",
      "{'loss': 0.4408, 'grad_norm': 18.46957015991211, 'learning_rate': 4.3e-05, 'epoch': 1.72}\n",
      "{'loss': 0.5235, 'grad_norm': 8.258384704589844, 'learning_rate': 4.4000000000000006e-05, 'epoch': 1.76}\n",
      "{'loss': 0.457, 'grad_norm': 10.030428886413574, 'learning_rate': 4.5e-05, 'epoch': 1.8}\n",
      "{'loss': 0.492, 'grad_norm': 36.285255432128906, 'learning_rate': 4.600000000000001e-05, 'epoch': 1.84}\n",
      "{'loss': 0.7256, 'grad_norm': 10.344809532165527, 'learning_rate': 4.7e-05, 'epoch': 1.88}\n",
      "{'loss': 0.5002, 'grad_norm': 2.7224910259246826, 'learning_rate': 4.8e-05, 'epoch': 1.92}\n",
      "{'loss': 0.4905, 'grad_norm': 44.00798034667969, 'learning_rate': 4.9e-05, 'epoch': 1.96}\n",
      "{'loss': 0.8091, 'grad_norm': 11.671283721923828, 'learning_rate': 5e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139fbf42d4e5478b9695841e45032e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5603524446487427, 'eval_runtime': 149.6435, 'eval_samples_per_second': 6.683, 'eval_steps_per_second': 0.421, 'epoch': 2.0}\n",
      "{'loss': 0.4884, 'grad_norm': 1.807286024093628, 'learning_rate': 4.8e-05, 'epoch': 2.04}\n",
      "{'loss': 0.6364, 'grad_norm': 9.66075325012207, 'learning_rate': 4.600000000000001e-05, 'epoch': 2.08}\n",
      "{'loss': 0.5098, 'grad_norm': 4.2256011962890625, 'learning_rate': 4.4000000000000006e-05, 'epoch': 2.12}\n",
      "{'loss': 0.5304, 'grad_norm': 8.858053207397461, 'learning_rate': 4.2e-05, 'epoch': 2.16}\n",
      "{'loss': 0.4711, 'grad_norm': 43.19972610473633, 'learning_rate': 4e-05, 'epoch': 2.2}\n",
      "{'loss': 0.414, 'grad_norm': 18.895606994628906, 'learning_rate': 3.8e-05, 'epoch': 2.24}\n",
      "{'loss': 0.555, 'grad_norm': 11.44795036315918, 'learning_rate': 3.6e-05, 'epoch': 2.28}\n",
      "{'loss': 0.4163, 'grad_norm': 18.696142196655273, 'learning_rate': 3.4000000000000007e-05, 'epoch': 2.32}\n",
      "{'loss': 0.3911, 'grad_norm': 4.72122859954834, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.36}\n",
      "{'loss': 0.3557, 'grad_norm': 4.1265549659729, 'learning_rate': 3e-05, 'epoch': 2.4}\n",
      "{'loss': 0.4378, 'grad_norm': 9.350801467895508, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.44}\n",
      "{'loss': 0.4401, 'grad_norm': 11.450845718383789, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.48}\n",
      "{'loss': 0.4044, 'grad_norm': 1.6751025915145874, 'learning_rate': 2.4e-05, 'epoch': 2.52}\n",
      "{'loss': 0.3055, 'grad_norm': 26.602493286132812, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.56}\n",
      "{'loss': 0.3779, 'grad_norm': 10.428547859191895, 'learning_rate': 2e-05, 'epoch': 2.6}\n",
      "{'loss': 0.3095, 'grad_norm': 11.588456153869629, 'learning_rate': 1.8e-05, 'epoch': 2.64}\n",
      "{'loss': 0.3718, 'grad_norm': 5.097376823425293, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.68}\n",
      "{'loss': 0.3111, 'grad_norm': 13.532947540283203, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.72}\n",
      "{'loss': 0.3953, 'grad_norm': 6.29699182510376, 'learning_rate': 1.2e-05, 'epoch': 2.76}\n",
      "{'loss': 0.4657, 'grad_norm': 5.282459259033203, 'learning_rate': 1e-05, 'epoch': 2.8}\n",
      "{'loss': 0.482, 'grad_norm': 8.88603687286377, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.84}\n",
      "{'loss': 0.4338, 'grad_norm': 10.382367134094238, 'learning_rate': 6e-06, 'epoch': 2.88}\n",
      "{'loss': 0.3479, 'grad_norm': 7.7754411697387695, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.92}\n",
      "{'loss': 0.3283, 'grad_norm': 4.970626354217529, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.96}\n",
      "{'loss': 0.2961, 'grad_norm': 18.039838790893555, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac242f6fe503426dadffff453b633918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.38916173577308655, 'eval_runtime': 175.7527, 'eval_samples_per_second': 5.69, 'eval_steps_per_second': 0.358, 'epoch': 3.0}\n",
      "{'train_runtime': 31385.4272, 'train_samples_per_second': 0.382, 'train_steps_per_second': 0.024, 'train_loss': 0.5389129981994629, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.5389129981994629, metrics={'train_runtime': 31385.4272, 'train_samples_per_second': 0.382, 'train_steps_per_second': 0.024, 'total_flos': 789340253184000.0, 'train_loss': 0.5389129981994629, 'epoch': 3.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the models\n",
    "trainer_roberta.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1933f5721de4a8091f471378bab813b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0539, 'grad_norm': 32.7119026184082, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.04}\n",
      "{'loss': 0.9647, 'grad_norm': 17.651409149169922, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.08}\n",
      "{'loss': 0.9046, 'grad_norm': 17.554433822631836, 'learning_rate': 3e-06, 'epoch': 0.12}\n",
      "{'loss': 0.8885, 'grad_norm': 24.23990821838379, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.16}\n",
      "{'loss': 0.7815, 'grad_norm': 14.04226016998291, 'learning_rate': 5e-06, 'epoch': 0.2}\n",
      "{'loss': 0.8017, 'grad_norm': 19.474647521972656, 'learning_rate': 6e-06, 'epoch': 0.24}\n",
      "{'loss': 0.7377, 'grad_norm': 13.498127937316895, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.28}\n"
     ]
    }
   ],
   "source": [
    "trainer_xlnet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RoBERTa\n",
    "predictions_roberta = trainer_roberta.predict(test_dataset_roberta)\n",
    "y_pred_roberta = np.argmax(predictions_roberta.predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate XLNet\n",
    "predictions_xlnet = trainer_xlnet.predict(test_dataset_xlnet)\n",
    "y_pred_xlnet = np.argmax(predictions_xlnet.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions\n",
    "np.save('roberta_predictions.npy', y_pred_roberta)\n",
    "np.save('xlnet_predictions.npy', y_pred_xlnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "def plot_training_loss(trainer, model_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(trainer.state.log_history, label='Training Loss')\n",
    "    plt.title(f'{model_name} Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_loss(trainer_roberta, 'RoBERTa')\n",
    "plot_training_loss(trainer_xlnet, 'XLNet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evaluation metrics\n",
    "def plot_evaluation_metrics(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    metrics = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "        'Score': [accuracy, precision, recall, f1]\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=metrics, x='Metric', y='Score')\n",
    "    plt.title(f'{model_name} Evaluation Metrics')\n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Score')\n",
    "    plt.show()\n",
    "\n",
    "plot_evaluation_metrics(y_test_numeric, y_pred_roberta, 'RoBERTa')\n",
    "plot_evaluation_metrics(y_test_numeric, y_pred_xlnet, 'XLNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
